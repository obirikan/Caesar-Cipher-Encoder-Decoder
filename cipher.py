from Tokenizer.main import Transformer

input_sentence = 'xuo jxuhu! jxyi yi qd unqcfbu ev q squiqh syfxuh. muhu oek qrbu je tusetu yj? y xefu ie! iudt cu q cuiiqwu rqsa myjx jxu iqcu evviuj!'
encoded, decoded = Transformer(input_sentence)
print(f"Original: {input_sentence}")
print(f"Encoded: {encoded}")
print(f"Decoded: {decoded}")








